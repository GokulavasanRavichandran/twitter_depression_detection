# -*- coding: utf-8 -*-
"""Emotional_Intensity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uE5j8O2fPCNpv-Z6pNXojy_mQ25X7gAv

# Non_depressive data collections from public databases

Here we try to get some neutral as well as emotional data from the competition (SemEval-2018 Task 1: Affect in Tweets (AIT-2018)) (https://competitions.codalab.org/competitions/17751#learn_the_details)

There are various subtasks and datasets. We decided to use the one with annotated Emotional Intensity, which is of Gold standard. This will allow us to be more specific about the type and range of emotions to include in our dataset, and more confident of the depth of its emotional content since it has been professionally annotated.

This notebook shows our analytical process in making the decision for inclusion. We try to find distinguishable patterns for each category in order to draw generalisaton based on their intensity score.

## Loading datasets
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/MyDrive/NLP/Depression_Detection/Data_Gathering_public_databases')

# Commented out IPython magic to ensure Python compatibility.
## Loading required libraries

import pandas as pd
from matplotlib import pyplot as plt
from statsmodels.graphics.gofplots import qqplot
import numpy as np
# %matplotlib inline

## Loading datasets
df_anger = pd.read_csv("/content/drive/MyDrive/NLP/Depression_Detection/Data_Gathering_public_databases/2018-EI-reg-En-anger-test-gold.txt", delimiter="\t")
df_fear = pd.read_csv("/content/drive/MyDrive/NLP/Depression_Detection/Data_Gathering_public_databases/2018-EI-reg-En-fear-test-gold.txt", delimiter="\t")
df_joy = pd.read_csv("/content/drive/MyDrive/NLP/Depression_Detection/Data_Gathering_public_databases/2018-EI-reg-En-joy-test-gold.txt", delimiter="\t")
df_sadness = pd.read_csv("/content/drive/MyDrive/NLP/Depression_Detection/Data_Gathering_public_databases/2018-EI-reg-En-sadness-test-gold.txt", delimiter="\t")

pd.set_option('display.max_colwidth', -1)

df_anger.head()

df_anger.shape

df_fear.head()

df_fear.shape

df_joy.head()

df_joy.shape

df_sadness.head()

df_sadness.shape

"""## Data Cleaning

Data cleaning is one of the essential steps because without a proper cleaning procedure you will have errors in your analysis and eventually your data-driven results. Here I try to eliminate duplicates tweets by using the Primary key ('ID'), checked for empty rows and replaced “NaN” if there is any.
"""

## Check the data type of each column
df_anger.dtypes.to_frame().rename(columns={0:'data_type'})

df_fear.dtypes.to_frame().rename(columns={0:'data_type'})

df_joy.dtypes.to_frame().rename(columns={0:'data_type'})

df_fear.dtypes.to_frame().rename(columns={0:'data_type'})

## Finding unique values in each column
for col in df_anger:
    print("There are ", len(df_anger[col].unique()), "unique values in ", col)

for col in df_fear:
    print("There are ", len(df_fear[col].unique()), "unique values in ", col)

for col in df_joy:
    print("There are ", len(df_joy[col].unique()), "unique values in ", col)

for col in df_sadness:
    print("There are ", len(df_sadness[col].unique()), "unique values in ", col)

"""**Comment:** There are no duplicates in our ID column for each datasets."""

## Find the number of Null values in each columns
df_anger.isnull().sum().to_frame().rename(columns={0:'Null values'})

df_fear.isnull().sum().to_frame().rename(columns={0:'Null values'})

df_joy.isnull().sum().to_frame().rename(columns={0:'Null values'})

df_sadness.isnull().sum().to_frame().rename(columns={0:'Null values'})

"""**Comments:** We don't have any missing values in our datasets.

## Exploratory Data Analysis
"""

## Let's check the distribution of our datasets
print(df_anger.describe())

## Histogram of our datasets

plt.hist(df_anger["Intensity Score"], bins=10)
plt.title('Anger')
plt.xlabel('Anger intensity')
plt.ylabel('Frequency')

print(df_fear.describe())

## Histogram of our datasets

plt.hist(df_fear["Intensity Score"], bins=10)
plt.title('Fear')
plt.xlabel('Fear intensity')
plt.ylabel('Frequency')

print(df_joy.describe())

## Histogram of our datasets

plt.hist(df_joy["Intensity Score"], bins=10)
plt.title('Joy')
plt.xlabel('Joy intensity')
plt.ylabel('Frequency')

print(df_sadness.describe())

## Histogram of our datasets

plt.hist(df_sadness["Intensity Score"], bins=10)
plt.title('Sadness')
plt.xlabel('Sadness intensity')
plt.ylabel('Frequency')

"""**Comments:** It seems that large portion of the tweets have an emotional intensity close to zero, so we have to first figure out explore close to zero intensities and see if they are worthy to keep for our purpose."""

## Exploring low intesity tweets

print(df_anger["Intensity Score"].value_counts().head())
print(df_fear["Intensity Score"].value_counts().head())
print(df_joy["Intensity Score"].value_counts().head())
print(df_sadness["Intensity Score"].value_counts().head())

## Seperating the zero intensity data from the rest to explore them further
df_anger0 = df_anger[df_anger["Intensity Score"] == 0]
df_fear0 = df_fear[df_fear["Intensity Score"] == 0]
df_joy0 = df_joy[df_joy["Intensity Score"] == 0]
df_sadness0 = df_sadness[df_sadness["Intensity Score"] == 0]

## Randomly choose 10 tweets from each datasets
print(df_anger0.sample(n=10, replace=True).Tweet,'\n', df_fear0.sample(n=10, replace=True).Tweet, '\n',
      df_joy0.sample(n=10, replace=True).Tweet, '\n', df_sadness0.sample(n=10, replace=True).Tweet)

"""**Comments:** The tweets seems to be the same for all our datasets and are not representing the true emotions; consequently might cause some bias in our model training step. Therefore, it would be better to exclude them."""

## Creating datasets without zero emotional intensity

newdf_anger = df_anger[df_anger["Intensity Score"] != 0]
newdf_fear = df_fear[df_fear["Intensity Score"] != 0]
newdf_joy = df_joy[df_joy["Intensity Score"] != 0]
newdf_sadness = df_sadness[df_sadness["Intensity Score"] != 0]

## Let's check the distribution of our new datasets

print(newdf_anger.describe())

## Histogram of our datasets

plt.hist(newdf_anger["Intensity Score"], bins=10)
plt.title('Anger')
plt.xlabel('Anger intensity')
plt.ylabel('Frequency')

# q-q plot
qqplot(newdf_anger["Intensity Score"], line='s')
plt.show()

print(newdf_fear.describe())

## Histogram of our datasets

plt.hist(newdf_fear["Intensity Score"], bins=10)
plt.title('Fear')
plt.xlabel('Fear intensity')
plt.ylabel('Frequency')

# q-q plot
qqplot(newdf_fear["Intensity Score"], line='s')
plt.show()

print(newdf_joy.describe())

## Histogram of our datasets

plt.hist(newdf_joy["Intensity Score"], bins=10)
plt.title('Joy')
plt.xlabel('Joy intensity')
plt.ylabel('Frequency')

# q-q plot
qqplot(newdf_joy["Intensity Score"], line='s')
plt.show()

print(newdf_sadness.describe())

## Histogram of our datasets

plt.hist(newdf_sadness["Intensity Score"], bins=10)
plt.title('Sadness')
plt.xlabel('Sadness intensity')
plt.ylabel('Frequency')

# q-q plot
qqplot(newdf_sadness["Intensity Score"], line='s')
plt.show()

"""**Comments:** Each dataset has almost 1000 records without zero emotional intensity which are all noramlly distributed (Gaussian distribution) based on their histograms and qq-plots"""

